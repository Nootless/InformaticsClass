{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7414f869-4ebc-4463-9d4c-196188210d02",
   "metadata": {},
   "source": [
    "### HW 1 - Part 2 - Spam Classifier ###\n",
    "\n",
    "Please rename the file with your \"First_Last_Name_HW1_Part2.ipynb\".<br><br>Please write the relevant code in the respective cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263cbbba-ca31-4244-81a1-9484cc219928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the modules you will need in completing this part\n",
    "# I have populated the warnings module to avoid unnecessary warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a5a2b0-be56-4ec7-a4a2-beaf1fbf0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "   class                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Read in the data from 'SMS_Spam.csv' and check\n",
    "# If you want to see what a row of data looks like print(data['message'].loc[9]\n",
    "data = pd.read_csv('sms_spam.csv')\n",
    "print(f'Original:\\n {data.head()}')\n",
    "# print(data['message'].loc[9])\n",
    "# print(data['message'][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33153638-f510-483e-8bb0-bfaea85e8d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Case Emails:\n",
      "  class                                            message\n",
      "0   ham  go until jurong point, crazy.. available only ...\n",
      "1   ham                      ok lar... joking wif u oni...\n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor... u c already then say...\n",
      "4   ham  nah i don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Convert all text in the dataframe column \"message\" to lower case\n",
    "# Check it has been done\n",
    "data['message'] = data['message'].str.lower()\n",
    "print(f'Lower Case Emails:\\n{data.head()}')\n",
    "# print(data['message'][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7580a271-b51e-48b9-9576-fbdac34b93d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Alphanumeric:\n",
      "   class                                            message\n",
      "0   ham  go until jurong point crazy available only in ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3   ham        u dun say so early hor u c already then say\n",
      "4   ham  nah i dont think he goes to usf he lives aroun...\n"
     ]
    }
   ],
   "source": [
    "# Remove non alphanumeric characters in the dataframe column \"message\"\n",
    "# Use apply and lambda as shown in examples in lectures\n",
    "data['message'] = data['message'].apply(lambda x: \"\".join([word for word in x if (word.isalnum() or word.isspace())]))\n",
    "print(f'No Alphanumeric:\\n {data.head()}')\n",
    "# print(data['message'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e8eedc-fa97-4d17-af16-9d28328b8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c6839c-e1d1-4e35-9bc5-22cacd19f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Data:\n",
      "   class                                            message\n",
      "0   ham  go until jurong point crazy available only in ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3   ham        u dun say so early hor u c already then say\n",
      "4   ham  nah i dont think he go to usf he life around h...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize text in the dataframe column \"message\"\n",
    "# Use apply and lambda as shown in examples in lectures\n",
    "# Check it worked\n",
    "data['message'] = data['message'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "print(f'Lemmatized Data:\\n {data.head()}')\n",
    "# print(data['message'][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36dacdc-ff0b-4cea-b22a-defffeecff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TF-IDF vectorizer\n",
    "# Make sure you set the parameter - stop_words='english' \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfvectorizer = TfidfVectorizer(stop_words='english')\n",
    "#analyzer='word'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290f22d2-6f42-4389-857a-fb91a2f30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in X and y datasets\n",
    "# X will be the 'message' column, y will be the 'class' column\n",
    "X = data['message']\n",
    "y = data['class']\n",
    "# print(f'X:\\n{X}')\n",
    "# print(f'y:\\n{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e76a565-1181-43a9-ae6e-0831f153d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "# Set random State = 20 and test_size = 0.15\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,random_state=20, test_size=.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c850ec-a8cb-4f48-b6d7-b3d6f5dfe48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the train data with the TF-IDF vectorizer and then\n",
    "# Transform train and test data to tfidf matrices\n",
    "tfvectorizer.fit(x_train)\n",
    "tfidf_train = tfvectorizer.transform(x_train)\n",
    "tfidf_test = tfvectorizer.transform(x_test)\n",
    "#print(tfidf_train)\n",
    "#print(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d27f4ed3-4cba-4cd8-b213-de5fadb03874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train and test matrices to dense matrices\n",
    "# Use 'todense()' or 'toarray()'\n",
    "tfidf_train = tfidf_train.todense()\n",
    "tfidf_test = tfidf_test.todense()\n",
    "# print(f'Fit data: {tfidf_fit}')\n",
    "# print(f'Test data: {tfidf_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18dd0873-9fcd-42cd-9690-0c325e0d51dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model: 96.7741935483871%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[723   0]\n",
      " [ 27  87]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Multinomial Naive Bayes classifier\n",
    "# Fit a model\n",
    "# Predict on test set\n",
    "# Print the accuracy and confusion matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(tfidf_train,y_train)\n",
    "y_r = mnb.predict(tfidf_test)\n",
    "print(f'Accuracy of Model: {accuracy_score(y_test,y_r) * 100}%\\n')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,y_r)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a38095e-a2a1-4ac1-a789-f39aae835f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is OPTIONAL\n",
    "\n",
    "# Identify the Most Powerful Features using this function\n",
    "# Print the 10 most important features.\n",
    "\n",
    "def get_most_important_features(vectorizer, classifier, n=None):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_features = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "    for coef, feat in top_features:\n",
    "        print(coef, feat)\n",
    "        \n",
    "        \n",
    "# Call the function here \n",
    "# get_most_important_features(tfvectorizer, mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51945444-22eb-4b79-bbba-4d371616b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Data on which you need to predict whether it is ham or spam\n",
    "# There are 6 documents in this list\n",
    "\n",
    "email = [\"Hello George, how about a game of tennis tomorrow?\",\n",
    "         \"freemsg for love birds for free membership use wap link\",\n",
    "         \"We offer free viagra!!! Click here now!!!\",\n",
    "         \"Dear Sara, I prepared the annual report.\",\n",
    "         \"Hi David, will we go for cinema tonight?\",\n",
    "         \"Free entry EPL cup tickets!!!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0097766-bd9e-48cb-a393-a274f9195b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Case List:\n",
      "['hello george, how about a game of tennis tomorrow?', 'freemsg for love birds for free membership use wap link', 'we offer free viagra!!! click here now!!!', 'dear sara, i prepared the annual report.', 'hi david, will we go for cinema tonight?', 'free entry epl cup tickets!!!']\n"
     ]
    }
   ],
   "source": [
    "# Usually you will need to apply all the preprocessing steps you did on the original dat also on this\n",
    "# To keep it simple - Create a new list which has these 6 documents but just converted to lower case\n",
    "# Check it is done\n",
    "email_list_lower = [x.lower() for x in email]\n",
    "print(f'Lower Case List:\\n{email_list_lower}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4baa9f86-8edb-4bbb-8915-bf400016d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model: 83.33333333333334%\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3 0]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Use the same TF-IDF object you fitted the train data with and transform the new list\n",
    "# Print predictions using the earlier fitted multinomial model \n",
    "# If you get - ['ham','spam','spam','ham','ham','spam'] then it would be 100% correct\n",
    "# Not critical that you get 100% results - more important that it works\n",
    "y_email_a = ['ham','spam','spam','ham','ham','spam']\n",
    "\n",
    "email_lower_tf = tfvectorizer.transform(email_list_lower)\n",
    "email_lower_tf = email_lower_tf.todense()\n",
    "y_email_r = mnb.predict(email_lower_tf)\n",
    "\n",
    "print(f'Accuracy of Model: {accuracy_score(y_email_a, y_email_r) * 100}%\\n')\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(y_email_a, y_email_r)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7d4ea-20f1-4e82-8a20-8ed00d41230e",
   "metadata": {},
   "source": [
    "#### Questions - Please answer in a Markdown Cell below ####\n",
    "\n",
    "1. What were your challenges in completing this part of the HW?\n",
    "\n",
    "2. What were your biggest learnings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b367b8-f7e2-4006-9b0c-9f2472cf1235",
   "metadata": {},
   "source": [
    "1. My biggest challenge was getting the lambdas to work. I had never been good with lambdas to begin with, so it was a bit difficult to relearn and apply it. Another issue I had was confirming whether my models were right. While I did get high accuracy, I wasn't sure exactly if these numbers were in the ball park of what should be expected.\n",
    "2. I feel that my biggest experience has been learning to work through the process of pre-processing and applying the models. While I probably could not write these functions from scratch, I could definitely see going through these steps by myself. Another thing was lambda expressions, which explained above, I was not very good at. Lastly, I feel that I have a better grasp at how different models are better for different things, as seen in part 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
